{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "entity_typing_data_parsing_BALANCED_ONTONOTES.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKL7Wwcj3pSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9bd442-3766-4e91-b1e2-ed46815494b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "base_dir = root_dir + \"tesi_magistrale/datasets/to_zip/\"\n",
        "path_corpus_data = root_dir + \"tesi_magistrale/datasets/corpus_data_folder/\"\n",
        "balanced_ontonotes_path = base_dir + \"balanced_ontonotes/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm6RoPKn3IyB"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(balanced_ontonotes_path + \"train.json\", 'r') as inp:\n",
        "    examples_train = [json.loads(l) for l in inp.readlines()]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-66bcSCfxoIo",
        "outputId": "2692a7fe-332f-4088-b581-b07fa2970e1a"
      },
      "source": [
        "print(\"il dataset train contiene \",len(examples_train), \"elementi\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "il dataset train contiene  249592 elementi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1R03kY2dKW"
      },
      "source": [
        "entire_dataset_list = examples_train"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mv7oiWh_RcM",
        "outputId": "6ba5f9b6-c785-446c-b41e-b43c5033d622"
      },
      "source": [
        "phrases = []\n",
        "restricted_list = []\n",
        "for i in range (0,10):\n",
        "  if (len(entire_dataset_list[i]['y_str'])>1):\n",
        "    for k in range(0,len(entire_dataset_list[i]['y_str'])):\n",
        "      phrase = []\n",
        "      phrase = entire_dataset_list[i]['left_context_token'] + entire_dataset_list[i]['y_str'][k].upper().split() + entire_dataset_list[i]['right_context_token']\n",
        "      phrases.append(phrase)\n",
        "      restricted_list.append(entire_dataset_list[i]['y_str'][k].upper())\n",
        "  else:\n",
        "    phrase = []\n",
        "    phrase = entire_dataset_list[i]['left_context_token'] + entire_dataset_list[i]['y_str'][0].upper().split() + entire_dataset_list[i]['right_context_token']\n",
        "    phrases.append(phrase)\n",
        "    restricted_list.append(entire_dataset_list[i]['y_str'][0].upper())\n",
        "\n",
        "print(phrases)\n",
        "print(restricted_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['OK', ',', 'we', \"'ll\", 'call', 'him', 'General', '/PERSON', '.'], ['/PERSON/TITLE', 'that', 'was', ',', 'the', 'only', 'way', 'you', 'could', 'overcome', 'them', 'and', 'get', 'the', 'upper', 'hand', '.'], ['/PERSON', 'that', 'was', ',', 'the', 'only', 'way', 'you', 'could', 'overcome', 'them', 'and', 'get', 'the', 'upper', 'hand', '.'], ['Because', ',', 'there', 'was', 'no', 'way', ',', 'if', 'you', 'were', \"n't\", 'tough', 'that', 'was', 'it', ',', ',', 'how', 'could', 'you', '/PERSON/TITLE', '.'], ['Because', ',', 'there', 'was', 'no', 'way', ',', 'if', 'you', 'were', \"n't\", 'tough', 'that', 'was', 'it', ',', ',', 'how', 'could', 'you', '/PERSON', '.'], ['/PERSON/TITLE', 'you', 'picked', 'up', 'the', 'shovel', 'and', 'rushed', 'them', '.'], ['/PERSON', 'you', 'picked', 'up', 'the', 'shovel', 'and', 'rushed', 'them', '.'], ['All', 'of', 'a', 'sudden', 'nobody', 'was', 'making', 'a', '/PERSON/TITLE', '.'], ['All', 'of', 'a', 'sudden', 'nobody', 'was', 'making', 'a', '/PERSON', '.'], ['Nopbody', 'went', 'to', 'get', 'the', '/PERSON/TITLE', 'or', 'somebody', \"'s\", 'father', '?'], ['Nopbody', 'went', 'to', 'get', 'the', '/PERSON', 'or', 'somebody', \"'s\", 'father', '?'], ['I', 'remember', 'one', 'time', '--', 'Mm', ',', 'also', ',', 'when', 'I', 'was', 'in', 'fifth', 'or', 'sixth', 'grade', ',', 'someone', 'yelled', 'out', ',', 'Down', 'with', '/LOCATION', 'Zhengming', ',', 'he', 'was', 'yelling', 'at', 'me', ',', 'and', 'I', 'had', 'to', 'fight', 'them', '.'], [',', 'the', 'factory', ',', ',', 'on', 'the', 'street', ',', 'somebody', 'had', 'written', 'in', 'huge', 'letters', '/LOCATION', 'with', 'Ye', 'Zhengming', ',', ',', 'the', 'slogan', '.'], [',', 'the', 'factory', ',', ',', 'on', 'the', 'street', ',', 'somebody', 'had', 'written', 'in', 'huge', 'letters', 'Down', 'with', '/LOCATION', 'Zhengming', ',', ',', 'the', 'slogan', '.'], ['I', 'got', 'accustomed', 'to', 'it', 'while', 'passing', 'by', 'at', 'that', 'time', ',', 'and', 'now', 'when', 'I', 'think', 'about', 'it', ',', 'it', 'seems', 'very', 'much', 'like', 'a', '/PERSON/TITLE', 'play', '.'], ['I', 'got', 'accustomed', 'to', 'it', 'while', 'passing', 'by', 'at', 'that', 'time', ',', 'and', 'now', 'when', 'I', 'think', 'about', 'it', ',', 'it', 'seems', 'very', 'much', 'like', 'a', '/PERSON', 'play', '.']]\n",
            "['/PERSON', '/PERSON/TITLE', '/PERSON', '/PERSON/TITLE', '/PERSON', '/PERSON/TITLE', '/PERSON', '/PERSON/TITLE', '/PERSON', '/PERSON/TITLE', '/PERSON', '/LOCATION', '/LOCATION', '/LOCATION', '/PERSON/TITLE', '/PERSON']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRU8kZvoIpRC"
      },
      "source": [
        "Save restricted list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTEbSFcNIsAx",
        "outputId": "0c36337f-b4a6-4007-d39b-22c1320d47e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "restricted_list = list(set(restricted_list))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/PERSON/TITLE', '/PERSON', '/LOCATION']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9OiojDpInvv"
      },
      "source": [
        "import pickle\n",
        "with open(path_corpus_data + 'restricted_list_balanced_ontonotes.pkl', 'wb') as fp:\n",
        "  pickle.dump(restricted_list, fp)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODPmdXCyJP7Z"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "with open(path_corpus_data + 'corpus_data_balanced_ontonotes.txt', 'w+') as fp:\n",
        "  for p in phrases:\n",
        "    fp.write(' '.join(p) + '\\n')"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}