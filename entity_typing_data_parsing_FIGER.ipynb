{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "entity_typing_data_parsing_FIGER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKL7Wwcj3pSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b644a785-e7b7-400f-a6a1-8dddb22057de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "base_dir = root_dir + \"tesi_magistrale/datasets/to_zip/\"\n",
        "path_corpus_data = root_dir + \"tesi_magistrale/datasets/corpus_data_folder/\"\n",
        "figer_path = base_dir + \"FIGER/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZRZN8DI6c9J"
      },
      "source": [
        "only_corpus=False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q8vyv_k6clo"
      },
      "source": [
        "if not only_corpus:\n",
        "  open(path_corpus_data + 'counters_figer.json', 'w').close()\n",
        "  open(path_corpus_data + 'types_counters_figer.json', 'w').close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm6RoPKn3IyB"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(figer_path + \"train.json\", 'r') as inp:\n",
        "    examples_train = [json.loads(l) for l in inp.readlines()]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-66bcSCfxoIo",
        "outputId": "2c2b901a-68bb-43f6-c389-044b0373fc3b"
      },
      "source": [
        "print(\"il dataset train contiene \",len(examples_train), \"elementi\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "il dataset train contiene  1505765 elementi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWJkJBNnBsXw"
      },
      "source": [
        "#ci sono 36 frasi che hanno le mentions vuote, le cancello\n",
        "indici = []\n",
        "for i,elem in enumerate(examples_train):\n",
        "  if len(elem['mentions'])==0:\n",
        "     indici.append(i)\n",
        "\n",
        "pop=0\n",
        "for num in indici:\n",
        "  del examples_train[num-pop]\n",
        "  pop+=1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7TBpSx8NI2"
      },
      "source": [
        "#examples_train=examples_train[0:10000]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkDsVLxg5M5k"
      },
      "source": [
        "entire_dataset_list = iter(examples_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDIiR7G16lhF"
      },
      "source": [
        "def get_permutations(i, threshold,generating_only_corpus):\n",
        "  # calcolo le permutazioni sulla base del dizionario, se sforano il treshold\n",
        "  # non le esplodo\n",
        "  d = sorted(i, key=operator.itemgetter(\"start\"))\n",
        "  d = {k: list(v) for k, v in itertools.groupby(d, key=operator.itemgetter(\"start\"))}\n",
        "\n",
        "  all_list=[]\n",
        "  for key, value in d.items():\n",
        "    all_list.append(value)\n",
        "\n",
        "  p = list(itertools.product(*all_list))\n",
        "  sum=0\n",
        "  #se sto generando solo il corpus e non mi interessa delle occorrenze\n",
        "  if generating_only_corpus:\n",
        "    for per in p:\n",
        "      prod=1\n",
        "      for i in per:\n",
        "        prod*= len(i)\n",
        "        #qui se prod già sfora treshold posso fermarmi tanto non mi serve sapere quante permutazioni genererebbe\n",
        "        if prod>treshold:\n",
        "          return [], [], sum\n",
        "      sum+=prod\n",
        "      #qui se sum già sfora treshold posso fermarmi tanto non mi serve sapere quante permutazioni genererebbe\n",
        "      if sum>treshold:\n",
        "        return [], [], sum\n",
        "    \n",
        "  else:\n",
        "    for per in p:\n",
        "      prod=1\n",
        "      for i in per:\n",
        "        prod*= len(i)\n",
        "        #qui se prod già sfora treshold posso fermarmi\n",
        "      sum+=prod\n",
        "      #qui se sum già sfora treshold posso fermarmi\n",
        "\n",
        "  if (sum>threshold):\n",
        "    return [], [], sum\n",
        "\n",
        "  else:\n",
        "    total_perm=[]\n",
        "    count_perm=0\n",
        "    for per in p:\n",
        "      mini_list = []\n",
        "      for lista_labels in per:\n",
        "        mini_list.append(lista_labels['labels'])\n",
        "\n",
        "      p2 = list(itertools.product(*mini_list))\n",
        "      total_perm.append(p2)\n",
        "\n",
        "    return total_perm.copy(), p.copy(), sum"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er5tAjcU6oRP",
        "outputId": "1686cd55-e989-4b84-f380-bf47ac47bbe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import operator\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import json\n",
        "treshold=20\n",
        "perm_a_priori=[]\n",
        "perm_effettive=[]\n",
        "sottostime=[]\n",
        "lunghezze={\n",
        "    'frasi_considerate':0,\n",
        "    'frasi_non_considerate':0,\n",
        "    'tot_permutazioni':0,\n",
        "}\n",
        "\n",
        "fp = open(path_corpus_data + 'corpus_data_entire_figer.txt', \"w\")\n",
        "restricted_set = {}\n",
        "\n",
        "for num,i in enumerate(tqdm(entire_dataset_list)):\n",
        "  if not only_corpus:\n",
        "    dizionario_permutations = {'pos':num, 'num': 0,'istanza':i}\n",
        "  istance = i['mentions'][:]\n",
        "  if len(istance)>1:\n",
        "    \n",
        "    permutations_2, permutations, num_permutations = get_permutations(istance, treshold,only_corpus)\n",
        "    if not only_corpus:\n",
        "      dizionario_permutations['num'] = num_permutations\n",
        "    if (not permutations):\n",
        "      lunghezze['frasi_non_considerate']+=1\n",
        "    else:\n",
        "      lunghezze['frasi_considerate']+=1\n",
        "      lunghezze['tot_permutazioni']+=num_permutations\n",
        "    iteri=0\n",
        "\n",
        "    for p_1 in permutations:\n",
        "      for p_2 in iter(permutations_2[iteri]):\n",
        "        \n",
        "        phrase = i['tokens'][:]\n",
        "        num_pop=0\n",
        "        for h in range(0, len(p_2)):\n",
        "          start = permutations[iteri][h]['start']\n",
        "          end = permutations[iteri][h]['end']\n",
        "\n",
        "          phrase[start-num_pop:end-num_pop] = [p_2[h].upper()]\n",
        "          if ((end-start)>1):\n",
        "            num_pop += (end - start) - 1\n",
        "          if not only_corpus:\n",
        "            if (p_2[h].upper() not in restricted_set):\n",
        "              restricted_set[p_2[h].upper()]=1\n",
        "            else:\n",
        "              restricted_set[p_2[h].upper()]+=1\n",
        "\n",
        "        \n",
        "        fp.write((remove_stopwords(' '.join(phrase)) + '\\n'))\n",
        "\n",
        "      iteri+=1\n",
        "    \n",
        "  else:\n",
        "    num_permutations=len(istance[0]['labels'])\n",
        "    if not only_corpus:\n",
        "      dizionario_permutations['num'] = num_permutations\n",
        "    lunghezze['tot_permutazioni']+= num_permutations\n",
        "    lunghezze['frasi_considerate']+=1\n",
        "\n",
        "    for k in istance[0]['labels']:\n",
        "      phrase = i['tokens'][:]\n",
        "      if (i['mentions'][0]['end'] - i['mentions'][0]['start'] ==1):          \n",
        "        phrase[i['mentions'][0]['start']] = k.upper()\n",
        "      else:\n",
        "        phrase[i['mentions'][0]['start']:i['mentions'][0]['end']] = [k.upper()]\n",
        "      if not only_corpus:\n",
        "        if (k.upper() not in restricted_set):\n",
        "          restricted_set[k.upper()]=1\n",
        "        else:\n",
        "          restricted_set[k.upper()]+=1\n",
        "\n",
        "      fp.write((remove_stopwords(' '.join(phrase)) + '\\n'))\n",
        "  if not only_corpus:\n",
        "    with open(path_corpus_data + 'counters_figer.json', 'a') as fp_1:\n",
        "        json.dump(dizionario_permutations, fp_1)\n",
        "\n",
        "fp.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1505729it [00:50, 29593.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLGaJb0k7FEL"
      },
      "source": [
        " if not only_corpus:\n",
        "  with open(path_corpus_data + 'types_counters_figer.json', 'a') as fp_3:\n",
        "        json.dump(restricted_set, fp_3)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou9V257s7Md-",
        "outputId": "57c5f0db-0d21-4774-d7ea-1c54fbf93c84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"frasi considerate con treshold {treshold}: {lunghezze['frasi_considerate']}\")\n",
        "print(f\"frasi non considerate con treshold {treshold}: {lunghezze['frasi_non_considerate']}\")\n",
        "print(f\"media di permutazioni per frase considerata {lunghezze['tot_permutazioni']/lunghezze['frasi_considerate']}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frasi considerate con treshold 20: 1219463\n",
            "frasi non considerate con treshold 20: 286266\n",
            "media di permutazioni per frase considerata 4.43028611774199\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}